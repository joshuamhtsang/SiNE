# Lessons Learned 1 - Real-Time Scene Viewer Design

## Context

During the design phase for implementing a real-time visualization system for SiNE emulations, an important architectural lesson emerged about avoiding redundant computation through proper understanding of existing system architecture.

## Initial AI-Generated Approach (FLAWED)

The initial plan generated by Claude Code proposed:

1. **New endpoint on channel server**: `POST /api/paths/all`
   - Would compute ray-traced paths **on demand** when the viewer queries
   - Each visualization update (every 1 second) would trigger expensive ray tracing computation
   - Estimated cost: ~100-500ms per link, repeated every second

2. **Architecture**:
   ```
   Viewer queries → Channel server computes paths → Returns paths → Viewer displays
                    (ray tracing happens here)
   ```

3. **Problem**: This approach completely ignored that the channel server **already computes paths** during normal emulation operation (when setting netem parameters for network links)

## User Guidance and Course Correction

The user identified the inefficiency with these key questions:

> "Can we just query the channel server for the latest node positions?"

> "I am concerned the current plan requires the channel server to re-compute paths, when the path should have been computed already by the channel server when the emulation asked for it to be computed (to update the netem parameter for the links)."

> "I think a better approach might be for the channel server to store in memory the latest set of nodes positions and the corresponding paths, and when viewer_live.ipynb queries for the scene and node positions, the channel server returns the node positions and associated paths."

## Revised Approach (CORRECT)

Based on user guidance, the architecture was completely redesigned:

1. **Cache paths during normal operation**: Store path details in memory when they're computed for netem configuration
   - Global cache: `_path_cache` and `_device_positions`
   - Updated in `/compute/single` endpoint after ray tracing
   - Zero additional computation cost

2. **New endpoint returns cached data**: `GET /api/visualization/state`
   - Returns pre-computed paths from cache
   - Instant response (~1-2ms instead of 100-500ms)
   - Paths guaranteed to be in sync with current netem parameters

3. **Architecture**:
   ```
   Emulation deployment → Channel server computes paths → Stores in cache
                          (ray tracing happens once)
                                      ↓
   Viewer queries → Channel server returns cached paths → Viewer displays
                    (instant, no computation)
   ```

## Key Lessons

### 1. Understand Existing Architecture Before Adding Features

**Problem**: Claude Code designed a feature in isolation without fully understanding how existing components work.

**Lesson**: Before proposing a new feature, thoroughly investigate:
- What computations already happen in the system?
- Where is data already being generated that could be reused?
- What data flows exist that could be tapped into?

In this case, the channel server was already computing paths for netem configuration. Simply caching this existing computation was far superior to creating a new on-demand computation path.

### 2. Question Redundant Computation

**Problem**: The initial plan would have performed the same expensive ray tracing computation twice:
1. Once for setting netem parameters (existing)
2. Again for visualization (proposed)

**Lesson**: When designing a feature that needs expensive computed data, always ask:
- Is this data already being computed elsewhere?
- Can we cache and reuse existing computation?
- What's the actual computational cost we're adding?

### 3. Human Domain Knowledge is Critical

**Problem**: Claude Code, despite having access to the entire codebase, missed the architectural opportunity to reuse existing path computations.

**Lesson**: AI code assistants can:
- Generate syntactically correct code
- Follow patterns they've seen
- Create plausible-looking architectures

But they may miss:
- System-wide optimization opportunities
- Understanding of what's computationally expensive
- Knowledge of what data is "already there" vs "needs to be computed"

**Human oversight remains essential** for architectural decisions.

### 4. Cost-Benefit of User Review

**Time investment**: User spent ~5 minutes reviewing the plan and asking clarifying questions

**Cost saved**:
- Avoided implementing a fundamentally inefficient architecture
- Prevented 100-500ms of redundant computation per visualization update
- Simplified the implementation (caching is simpler than on-demand computation)

**Lesson**: Even brief human review of AI-generated designs can catch major inefficiencies that would be expensive to fix later.

## Specific Implementation Insights

### What Changed

**Before (inefficient)**:
```python
# Viewer triggers new computation every 1 second
@app.post("/api/paths/all")
async def compute_all_paths(node_positions):
    paths = []
    for tx, rx in node_pairs:
        # EXPENSIVE: Ray tracing for each pair
        path = path_solver.solve(tx, rx)
        paths.append(path)
    return paths
```

**After (efficient)**:
```python
# Cache paths when already computing for netem
@app.post("/compute/single")
async def compute_single_channel(request):
    # ... existing ray tracing for netem ...
    path_details = engine.get_path_details()

    # NEW: Store in cache (negligible cost)
    _path_cache[link_id] = {
        "tx_position": tx_pos,
        "rx_position": rx_pos,
        "paths": convert_to_json(path_details)
    }

    return channel_response

# Viewer gets cached data (instant)
@app.get("/api/visualization/state")
async def get_visualization_state():
    return {
        "paths": list(_path_cache.values()),
        "devices": list(_device_positions.values())
    }
```

### Performance Impact

| Metric | Before (inefficient) | After (efficient) | Improvement |
|--------|---------------------|-------------------|-------------|
| Visualization query latency | 100-500ms per link | 1-2ms total | **~100-500x faster** |
| Redundant computation | Yes (100%) | None | **Eliminated** |
| Memory overhead | None | ~10-50KB per link | Negligible |
| Code complexity | High (new ray tracing path) | Low (simple cache) | Simpler |

## Applicability to Job Interviews

This example demonstrates several valuable qualities:

1. **Willingness to accept feedback**: Changed approach completely based on user input
2. **Understanding trade-offs**: Memory overhead vs computational savings
3. **System thinking**: Recognized data flow opportunities across components
4. **Performance awareness**: Understood cost of ray tracing (100-500ms)
5. **Iterative design**: Initial plan → user feedback → revised plan

When discussing this in interviews:
- **Situation**: Designing real-time visualization for wireless network emulation
- **Task**: Enable viewer to display ray-traced propagation paths
- **Action**: Initial AI-generated plan proposed on-demand path computation; reviewed with domain expert who identified existing path computations could be cached
- **Result**: Avoided 100-500ms of redundant computation per visualization update by caching existing data

## Conclusion

This design iteration illustrates that **AI code assistants are powerful tools but require human oversight** for architectural decisions. The user's domain knowledge and understanding of computational costs led to a dramatically better design than the AI's initial proposal.

**Key takeaway**: Always review AI-generated designs with the question: "Is there existing computation we can reuse?" The best code is often the code you don't have to write (or run) at all.




# Lessons Learned 2 - Containerlab bridge modes

Claude assumed that containerlab could create bridges, but it cannot. According to documentation here:

https://containerlab.dev/manual/kinds/bridge/

There are 2 options:

1. Pre-creation of bridges directly in the host namespace.
2. Container-Namespace Bridges

I suggested going for option 2, as this results in the network.yaml being a complete deployment description, without needing additional scripts outside of network.yaml.



# Lesson Learned 3 - Creating good integration tests is key to shaping the development

It was really hard getting SINR and MAC-level protocols (CSMA, TDMA) feature implemented, need lots of steering. But having integration tests codified the human requirements for SiNE. Claude Code goes around in circles due to catch-22's (around CSMA and carrier sensing range), Claude is not great at spatial reasoning.  I suggested lienar topology to bring out hidden node problem better.

I discovered errors in how 'ip route add' was being used my SiNE (it was using host only /32 suffix...). Kept prompting to refine the integration tests, so future issues are discovered earlier - this take domain knowledge to identify the common pitfalls in Linux networking.

Claude Code is so smart, but occasionally make basic errors.  "So smart, but so stupid, like Jan in Love is Blind".
